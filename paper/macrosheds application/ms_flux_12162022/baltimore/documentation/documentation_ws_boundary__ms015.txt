
This MacroSheds data product (ws_boundary__ms015) is a "linked product", meaning it was complete after munging and
did not require additional "derived-level" processing. As such, the munged product
was simply linked to our derived product directory and given a standard MacroSheds derived product code.

Source data were retrieved from the following web page(s), static file(s), or web-API endpoint(s):

https://portal.lternet.edu/nis/mapbrowse?packageid=knb-lter-and.3200.100

Using these notes, our code on GitHub, and assuming you have a good bit of R programming experience,
you will ideally be able to piece together exactly how ws_boundary__ms015 was generated and regenerate it yourself.
That said, automated documentation like this is bound to have some errors and missing details until
we get all the kinks worked out, and in any case assembling all the code below into a usable script
won't be an easy task. Please contact us at mail@macrosheds.org if you're having trouble
navigating our docs.

This is the "munge kernel" function for ws_boundary__3200:

process_1_3200 <- function (network, domain, prodname_ms, site_code, component) 
{
    rawfile <- glue("data/{n}/{d}/raw/{p}/{s}/{c}.zip", n = network, 
        d = domain, p = prodname_ms, s = site_code, c = component)
    rawdir <- glue("data/{n}/{d}/raw/{p}", n = network, d = domain, 
        p = prodname_ms)
    zipped_files <- unzip(zipfile = rawfile, exdir = rawdir, 
        overwrite = TRUE)
    projstring <- choose_projection(unprojected = TRUE)
    shape_paths <- glue("{d}/01m/{s}", d = rawdir, s = "BES Watershed Boundary Shapefiles")
    files <- list.files(shape_paths, full.names = T)
    shp_files <- grep(".shp", files, value = T)
    shp_files <- shp_files[!grepl(".shp.xml", shp_files)]
    full_site_code <- str_split_fixed(shp_files, "[/]", n = Inf)[, 
        8]
    full_site_code <- str_split_fixed(full_site_code, "[.]", 
        n = Inf)[, 1]
    for (i in 1:length(shp_files)) {
        site_code <- case_when(full_site_code == "Baisman_Run" ~ 
            "BARN", full_site_code == "Carroll_Park" ~ "GFCP", 
            full_site_code == "Dead_Run" ~ "DRKR", full_site_code == 
                "Glyndon" ~ "GFGL", full_site_code == "Gwynnbrook" ~ 
                "GFGB", full_site_code == "McDonogh" ~ "MCDN", 
            full_site_code == "Pond_Branch" ~ "POBR", full_site_code == 
                "Villa_Nova" ~ "GFVN")
        wb <- st_read(shp_files[i]) %>% mutate(site_code = !!site_code[i], 
            area = Area_m2/10000) %>% select(-Area_m2) %>% sf::st_transform(4326)
        write_ms_file(d = wb, network = network, domain = domain, 
            prodname_ms = prodname_ms, site_code = site_code[i], 
            level = "munged", shapefile = TRUE, link_to_portal = FALSE)
    }
    unlink(zipped_files)
    return()
}

These were the arguments to that function:

network = 'lter'
domain = 'baltimore'
prodname_ms = 'ws_boundary__3200'
site_code = <separately, each of: 'sitename_NA', with corresponding component>
component(s) = 
	for site: sitename_NA
		comp(s): GIS Shapefile, Spatial boundaries and land cover summaries for eight sub-watersheds of the Baltimore Ecosystem Study LTER

For most products, there would be (additional) kernel function code included here. For some ws_boundary products
like this one, we use MacroSheds' delineation tool, ms_delineate(), which can be found at
https://github.com/MacroSHEDS/data_processing/blob/master/src/global_helpers.R
Note that in a few cases, domains provide some but not all watershed boundary files, so you may have
to use these documentation files *and* ms_delineate() to fully reproduce our dataset.


---

Functions from external packages called inside the kernel function are either
referenced with `<package name>::<function>`, or are called from their aliases, defined in:

https://github.com/MacroSHEDS/data_processing/blob/master/src/function_aliases.R

For definitions of most MacroSheds functions called, see:

https://github.com/MacroSHEDS/data_processing/blob/master/src/global_helpers.R

Definitions not found there will be found in src/<network>/network_helpers.R or
src/<network>/<domain>/domain_helpers.R, where network is e.g. lter and domain
is e.g. hbef (Hubbard Brook Experimental Forest). For a catalogue of networks
and domains, download our site data table from the Data tab at macrosheds.org.

After all the kernels have completed their jobs, there is a whole suite of
post-processing steps, some of which further modify derived data. See
postprocess_entire_dataset() in global_helpers.R for a list of these.

Note that most MacroSheds functions are wrapped in a decorator function (handle_errors,
defined in global_helpers.R; see tinsel package for details). This decorator is not needed
to run the functions it wraps. To circumvent it, just make sure you don't load
function definitions using tinsel::source_decoratees. This would only happen if you
were to execute MacroSheds code line-by-line, starting from:

https://github.com/MacroSHEDS/data_processing/blob/master/src/acquisition_master.R

Also note that the return value of a munge kernel function may be additionally modified by
a munge engine function. Inside the body of the engine function, you can see where the
munge kernel is retrieved with get() and called via do.call(). Usually, the only additional munging
done by the munge engine (versus the munge kernel) would be to separate
a data file that contains many sites into individual data files of only one site each.
Munge engines are defined in:

https://github.com/MacroSHEDS/data_processing/blob/master/src/munge_engines.R

Finally, consider that you may clone our entire project from Github and get it running
on your own machine. Getting every component to run will require a file called config.json
at the top level of each project repository (data_processing/ and portal/) with your own
values instead of <...> for each of the following fields:

}
    "gmail_pw": "< >",
    "report_emails": ["<email1>", "<email2 etc>"],
    "variables_gsheet": "https://docs.google.com/spreadsheets/< >",
    "site_data_gsheet": "https://docs.google.com/spreadsheets/< >",
    "delineation_gsheet": "https://docs.google.com/spreadsheets/< >",
    "univ_prods_gsheet:": "https://docs.google.com/spreadsheets/< >",
    "name_variant_gsheet": "https://docs.google.com/spreadsheets/< >",
    "gee_login_<yourname>": "< >",
    "orcid_login_<yourname>": "< >",
    "orcid_pass_<yourname>": "< >",
}

Of course, you won't need connections to the ORCID database or google sheets in order to make headway.
For example, just set config_storage_location = 'local' in your call to ms_init in data_processing/src/acquisition_master.R
and gsheets becomes irrelevant. Our system is not fully set up to bypass the errors that would result from omitting
some of this config information, but workarounds (like commenting lines or inserting tryCatch blocks) should be
possible. This is something we'll be working on in later phases of the project.